{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJtK5bHjkEhx"
   },
   "source": [
    "# DIGI405 - Text Classification with Transformers\n",
    "\n",
    "Notebook developed by Karin Stahel.  \n",
    "\n",
    "⚠️ **Run this notebook in Google Colab - not in the DIGI405 JupyterHub**. You will want to change to a GPU/TPU accelerated runtime. To do this go to Runtime > Change runtime type. Read more about GPU/TPU availability in Google Colab [here](https://research.google.com/colaboratory/faq.html#gpu-availability).  \n",
    "\n",
    "This notebook fine-tunes the [\"distilbert-base-uncased\"](https://huggingface.co/distilbert/distilbert-base-uncased) model on the [20 newsgroups dataset](http://qwone.com/~jason/20Newsgroups/). DistilBERT base (uncased) is accessed through the 🤗 Hugging Face [transformers](https://huggingface.co/docs/hub/en/transformers) library.  \n",
    "\n",
    "Compare the performance of your fine-tuned transformer model with the model built using the [Text Classification Introduction notebook](https://github.com/polsci/text-classification-introduction).\n",
    "\n",
    "---\n",
    "\n",
    "**Remember:** Each time you change settings below, you need to rerun the following cells in order to implement the classification pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NB5SNu01kEh1"
   },
   "source": [
    "This cell imports installs a required library. If Colab prompts you to restart the session, do that and then continue running the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C5Dh1nOTlkrS",
    "outputId": "c4a9181b-9338-42e2-857e-19c598360f96"
   },
   "outputs": [],
   "source": [
    "# Install datasets\n",
    "!pip install datasets==2.14.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EcEI7E9hkEh1"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FogXcm6kkEh4"
   },
   "source": [
    "## Load corpus and set train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the category names\n",
    "categories = datasets.fetch_20newsgroups().target_names\n",
    "for category in categories:\n",
    "    print(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this chooses the categories to load\n",
    "cats = ['talk.politics.misc', 'talk.religion.misc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sg3F29gQkEh4"
   },
   "outputs": [],
   "source": [
    "# this downloads/loads the data\n",
    "# dataset = fetch_20newsgroups(subset='train', categories=cats)\n",
    "dataset = datasets.fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), categories=cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the train/test split - 0.2 is 80% for training, 20% for testing\n",
    "test_size = 0.2\n",
    "\n",
    "# do the train test split ...\n",
    "# X_train and X_test are the documents\n",
    "# y_train and y_test are the labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, \n",
    "                                                    test_size = test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Em7iN6u9lCkz"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zwJaff96jlA"
   },
   "source": [
    "## Fine-tune DistilBERT\n",
    "\n",
    "**Question:** Run this model and compare the results to the Bag of Words (Naive Bayes) model. Make note of some of the pros and cons of each and discuss with your neighbour or the tutors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0_pVvd_kEh_"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZCy2DA0-7-d"
   },
   "source": [
    "The following cell sets-up and trains the model, then returns the results of evaluation against the test set.\n",
    "\n",
    "Just run the cell, there is no need to change anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lZdojmL0kEh_",
    "outputId": "fe58f617-e9ae-4e66-e8e0-21941dc88da3"
   },
   "outputs": [],
   "source": [
    "print('\\n\\nWe further split our training set into a new training set and an evaluation set\\n\\n')\n",
    "docs_train_b, docs_eval, y_train_b, y_eval = train_test_split(X_train, y_train, test_size=0.2, random_state=None)\n",
    "\n",
    "# Set up DistilBERT model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=len(dataset.target_names))\n",
    "\n",
    "train_encodings = tokenizer(docs_train_b, truncation=True, padding=True, max_length=512)\n",
    "eval_encodings = tokenizer(docs_eval, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "train_dataset = Dataset.from_dict({'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask'], 'labels': y_train_b})\n",
    "eval_dataset = Dataset.from_dict({'input_ids': eval_encodings['input_ids'], 'attention_mask': eval_encodings['attention_mask'], 'labels': y_eval})\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset\n",
    ")\n",
    "\n",
    "print('\\n\\nTraining...please be patient 😉\\n')\n",
    "\n",
    "print(\"\"\"\n",
    "Key points:\n",
    "Convergence: Ideally, both training and validation loss should be decreasing in each epoch and converging to a lower value.\n",
    "Overfitting: If the training loss decreases while the validation loss increases, it indicates that the model is overfitting to the training data.\n",
    "Underfitting: If both training and validation loss remain high, it indicates the model is underfitting - it is not learning well from the data.\n",
    "\"\"\")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "def distilbert_classify(texts, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Classify new text using our fine-tuned DistilBERT model.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # check for cuda availability and set device\n",
    "    model = model.to(device) # move model to device\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512).to(device) # move inputs to device\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    return probabilities, probabilities.argmax(dim=-1).tolist()\n",
    "\n",
    "print('\\nNow see how the fine-tuned model performs on new data - our original test set')\n",
    "\n",
    "# Evaluate the model\n",
    "probabilities, hf_predictions = distilbert_classify(X_test, model, tokenizer)\n",
    "\n",
    "print(\"\\nDistilBERT Results:\")\n",
    "print(classification_report(y_test, hf_predictions, target_names=dataset.target_names))\n",
    "\n",
    "# Display confusion matrix\n",
    "cm = metrics.confusion_matrix(y_true=y_test, y_pred=hf_predictions, labels=range(len(dataset.target_names)))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=dataset.target_names)\n",
    "disp.plot(include_values=True, cmap='Blues', xticks_rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "siypopf-p95h",
    "outputId": "7789f3df-504d-4357-fbe7-17e39a318cb3"
   },
   "outputs": [],
   "source": [
    "# Identify and rank incorrectly classified documents\n",
    "incorrect_indices = [i for i, (true, pred) in enumerate(zip(y_test, hf_predictions)) if true != pred]\n",
    "incorrect_docs = [\n",
    "    (\n",
    "        i,  # Original index\n",
    "        X_test[i],\n",
    "        dataset.target_names[y_test[i]],  # Map true label to text\n",
    "        dataset.target_names[hf_predictions[i]],  # Map predicted label to text\n",
    "        probabilities[i].max().item()\n",
    "    )\n",
    "    for i in incorrect_indices\n",
    "]\n",
    "\n",
    "# Sort by the highest confidence in the incorrect class\n",
    "incorrect_docs_sorted = sorted(incorrect_docs, key=lambda x: x[4], reverse=True)\n",
    "df_incorrect = pd.DataFrame(incorrect_docs_sorted, columns=['Test set index', 'Document', 'True Label', 'Predicted Label', 'Confidence'])\n",
    "\n",
    "print(\"\\nIncorrectly Classified Documents\\n\")\n",
    "print(\"Confidence is the probability the model has assigned for the given document belonging to the predicted class.\\n\")\n",
    "display(df_incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HB9NpIV-YLT",
    "outputId": "fcdd602f-fdca-4af6-b364-a2aa04a35c53"
   },
   "outputs": [],
   "source": [
    "index_to_display = 1\n",
    "print(df_incorrect.loc[index_to_display, 'Document'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
